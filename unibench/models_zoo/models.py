"""
Copyright (c) Meta Platforms, Inc. and affiliates.
All rights reserved.
This source code is licensed under the license found in the
LICENSE file in the root directory of this source tree.
"""

import os
from unibench.models_zoo import register_model
from unibench.common_utils.constants import HUB_CACHE_DIR, CURRENT_DIR
from unibench.models_zoo.wrappers import *
import timm
import open_clip
import clip
from git import Repo
import sys
import torch

from unibench.models_zoo.wrappers.vlm import Chameleon, LVLModel, LlavaNext, PaliGemma


def load_blip(model_name, model_url, model_size="base", image_size=224, **kwargs):
    if not HUB_CACHE_DIR.joinpath("BLIP").exists():
        Repo.clone_from(
            "https://github.com/salesforce/BLIP.git", HUB_CACHE_DIR.joinpath("BLIP")
        )
    sys.path.append(str(HUB_CACHE_DIR.joinpath("BLIP")))
    from models.blip_itm import blip_itm

    os.chdir(str(HUB_CACHE_DIR.joinpath("BLIP")))

    model = blip_itm(
        pretrained=model_url,
        image_size=image_size,
        vit=model_size,
    )
    os.chdir(str(CURRENT_DIR))

    return BlipModel(
        model=model,
        model_name=model_name,
        tokenizer=model.tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        use_itm_head=True,
        input_resolution=image_size,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Llava",
    },
)
def paligemma_3b(model_name, **kwargs):
    from transformers import PaliGemmaForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "google/paligemma-3b-pt-224"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = PaliGemmaForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return PaliGemma(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        prompt='Describe the following image:',
        processor=processor,
        use_norm=False,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.size['width'],
        **kwargs
    )

@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Llava",
    },
)
def llava_1_5_7b(model_name, **kwargs):
    from transformers import LlavaForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "llava-hf/llava-1.5-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = LlavaForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return LVLModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.crop_size['width'],
        **kwargs
    )
    
@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Llava",
    },
)
def llava_1_5_13b(model_name, **kwargs):
    from transformers import LlavaForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "llava-hf/llava-1.5-13b-hf"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = LlavaForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return LVLModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.crop_size['width'],
        **kwargs
    )
    
@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Llava",
    },
)
def llava_1_7b(model_name, **kwargs):
    from transformers import LlavaForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "llava-hf/bakLlava-v1-hf"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = LlavaForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return LVLModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.crop_size['width'],
        **kwargs
    )
    
@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Llava Next",
    },
)
def llava_next_1_6_7b_vicuna(model_name, **kwargs):
    from transformers import LlavaNextForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "llava-hf/llama3-llava-next-8b-hf"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = LlavaNextForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 1

    return LlavaNext(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        use_norm=False,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.crop_size['width'],
        **kwargs
    )
    
@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Chameleon",
    },
)
def chameleon_7b(model_name, **kwargs):
    from transformers import ChameleonForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "facebook/chameleon-7b"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = ChameleonForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return Chameleon(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        use_norm=False,
        prompt='What do you see in this image?<image>',
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.crop_size['width'],
        **kwargs
    )
    
@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 7000,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "Chameleon",
    },
)
def blip2_7b(model_name, **kwargs):
    from transformers import Blip2ForConditionalGeneration
    from transformers import AutoTokenizer
    from transformers import AutoProcessor

    name = "Salesforce/blip2-opt-6.7b"
    tokenizer = AutoTokenizer.from_pretrained(name)
    model = Blip2ForConditionalGeneration.from_pretrained(
        name, low_cpu_mem_usage=True, torch_dtype=torch.float16
    ).cuda()
    processor = AutoProcessor.from_pretrained(name)
    kwargs['batch_per_gpu'] = 2

    return LVLModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        processor=processor,
        norm_mean=processor.image_processor.image_mean,
        norm_std=processor.image_processor.image_std,
        input_resolution=processor.image_processor.size['width'],
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 14,
        "model_size": 86,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT B 16 ",
    },
)
def blip_vitB16_14m(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth",
        model_size="base",
        image_size=224,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 307,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT L 16 ",
    },
)
def blip_vitL16_129m(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth",
        model_size="large",
        image_size=224,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 86,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT B 16 ",
    },
)
def blip_vitB16_129m(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth",
        model_size="base",
        image_size=224,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 86,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT B 16 ",
    },
)
def blip_vitB16_coco(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth",
        model_size="base",
        image_size=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 86,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT B 16 ",
    },
)
def blip_vitB16_flickr(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth",
        model_size="base",
        image_size=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 307,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT L 16 ",
    },
)
def blip_vitL16_coco(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth",
        model_size="large",
        image_size=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 129,
        "model_size": 307,
        "learning_objective": "BLIP",
        "architecture": "vit",
        "name": "BLIP ViT L 16 ",
    },
)
def blip_vitL16_flickr(model_name, **kwargs):
    return load_blip(
        model_name=model_name,
        model_url="https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth",
        model_size="large",
        image_size=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 4350,
        "learning_objective": "EVA02",
        "architecture": "vit",
        "name": "EVA02 ViT E 14",
    },
)
def eva02_vitE14_plus_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA02-E-14-plus", pretrained="laion2b_s9b_b144k"
    )

    tokenizer = open_clip.get_tokenizer("EVA02-E-14-plus")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 4350,
        "learning_objective": "EVA02",
        "architecture": "vit",
        "name": "EVA02 ViT E 14",
    },
)
def eva02_vitE14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA02-E-14", pretrained="laion2b_s4b_b115k"
    )

    tokenizer = open_clip.get_tokenizer("EVA02-E-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 307,
        "learning_objective": "EVA02",
        "architecture": "vit",
        "name": "EVA02 ViT L 14",
    },
)
def eva02_vitL14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA02-L-14", pretrained="merged2b_s4b_b131k"
    )

    tokenizer = open_clip.get_tokenizer("EVA02-L-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 86,
        "learning_objective": "EVA02",
        "architecture": "vit",
        "name": "EVA02 ViT B 16",
    },
)
def eva02_vitB16_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA02-B-16", pretrained="merged2b_s8b_b131k"
    )

    tokenizer = open_clip.get_tokenizer("EVA02-B-16")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 1011,
        "learning_objective": "EVA01",
        "architecture": "vit",
        "name": "EVA01 ViT g 14",
    },
)
def eva01_vitG14_plus_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA01-g-14-plus", pretrained="merged2b_s11b_b114k"
    )

    tokenizer = open_clip.get_tokenizer("EVA01-g-14-plus")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 1011,
        "learning_objective": "EVA01",
        "architecture": "vit",
        "name": "EVA01 ViT g 14",
    },
)
def eva01_vitG14_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "EVA01-g-14", pretrained="laion400m_s11b_b41k"
    )

    tokenizer = open_clip.get_tokenizer("EVA01-g-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 1843,
        "learning_objective": "CLIPA",
        "architecture": "vit",
        "name": "CLIPA ViT G 14",
    },
)
def clipa_vitbigG14(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-bigG-14-CLIPA", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViT-bigG-14-CLIPA")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=32,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 22,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-S",
    },
)
def vitamin_s_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-S", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-S")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 22,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-S-LTT",
    },
)
def vitamin_s_ltt_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-S-LTT", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-S-LTT")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 87,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-B",
    },
)
def vitamin_b_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-B", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-B")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 87,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-B-LTT",
    },
)
def vitamin_b_ltt_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-B-LTT", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-B-LTT")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L",
    },
)
def vitamin_l_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L2",
    },
)
def vitamin_l2_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L2", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L2")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L2-256",
    },
)
def vitamin_l2_256_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L2-256", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L2-256")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L2-336",
    },
)
def vitamin_l2_336_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L2-336", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L2-336")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 436,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-XL-256",
    },
)
def vitamin_xl_256_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-XL-256", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-XL-256")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 436,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-XL-336",
    },
)
def vitamin_xl_336_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-XL-336", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-XL-336")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 436,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-XL-384",
    },
)
def vitamin_xl_384_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-XL-384", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-XL-384")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L-256",
    },
)
def vitamin_l_256_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L-256", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L-256")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 333,
        "learning_objective": "ViTamin",
        "architecture": "vitamin",
        "name": "ViTamin-L-336",
    },
)
def vitamin_l_336_1b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViTamin-L-336", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViTamin-L-336")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 633,
        "learning_objective": "CLIPA",
        "architecture": "vit",
        "name": "CLIPA ViT H 14",
    },
)
def clipa_vitH14(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-H-14-CLIPA", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViT-H-14-CLIPA")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=32,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 307,
        "learning_objective": "CLIPA",
        "architecture": "vit",
        "name": "CLIPA ViT L 14",
    },
)
def clipa_vitL14(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14-CLIPA", pretrained="datacomp1b"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14-CLIPA")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=32,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 10000,
        "model_size": 307,
        "learning_objective": "Contrastive (sigmoid-based)",
        "architecture": "vit",
        "name": "SigLIP ViT L 16",
    },
)
def siglip_vitL16(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-16-SigLIP-256", pretrained="webli"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-16-SigLIP-256")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=64,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 10000,
        "model_size": 86,
        "learning_objective": "Contrastive (sigmoid-based)",
        "architecture": "vit",
        "name": "SigLIP ViT B 16",
    },
)
def siglip_vitB16(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16-SigLIP-256", pretrained="webli"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16-SigLIP-256")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=64,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2500,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT B 32",
    },
)
def openclip_vitB32_metaclip_fullcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32-quickgelu", pretrained="metaclip_fullcc"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT B 16",
    },
)
def openclip_vitB16_metaclip_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16-quickgelu", pretrained="metaclip_400m"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT B 32",
    },
)
def openclip_vitB32_metaclip_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32-quickgelu", pretrained="metaclip_400m"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "ViT B 32 GeLU",
    },
)
def openclip_vitB32_quickgelu_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32-quickgelu", pretrained="laion400m_e32"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "ViT B 32 GeLU",
    },
)
def openclip_vitB32_quickgelu_openai(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32-quickgelu", pretrained="openai"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2500,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT B 16",
    },
)
def openclip_vitB16_metaclip_fullcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16-quickgelu", pretrained="metaclip_fullcc"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT L 14",
    },
)
def openclip_vitL14_dfn2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14-quickgelu", pretrained="dfn2b"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT L 14",
    },
)
def openclip_vitL14_metaclip_400(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14-quickgelu", pretrained="metaclip_400m"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2500,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT L 14",
    },
)
def openclip_vitL14_metaclip_fullcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14-quickgelu", pretrained="metaclip_fullcc"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2500,
        "model_size": 633,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "MetaCLIP ViT H 14",
    },
)
def openclip_vitH14_metaclip_fullcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-H-14-quickgelu", pretrained="metaclip_fullcc"
    )

    tokenizer = open_clip.get_tokenizer("ViT-H-14-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 5000,
        "model_size": 633,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT H 14",
    },
)
def openclip_vitH14_dfn5b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-H-14-quickgelu", pretrained="dfn5b"
    )

    tokenizer = open_clip.get_tokenizer("ViT-H-14-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 88,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "OpenCLIP ConvNext",
    },
)
def openclip_convnext_base(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "convnext_base", pretrained="laion400m_s13b_b51k"
    )

    tokenizer = open_clip.get_tokenizer("convnext_base")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "clipHero",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "CLIP ViT B 32",
    },
)
def clip_vitB32(model_name, **kwargs):
    model, _ = clip.load("ViT-B/32")

    tokenizer = clip.tokenize

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 13,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT B 32",
    },
)
def openclip_vitB32_datacomp_s(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained="datacomp_s_s13m_b4k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 128,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT B 32",
    },
)
def openclip_vitB32_datacomp_m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained="datacomp_m_s128m_b4k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 12800,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT B 32",
    },
)
def openclip_vitB32_datacomp_xl(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained="datacomp_xl_s13b_b90k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 12800,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT B 16",
    },
)
def openclip_vitB16_datacomp_xl(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16", pretrained="datacomp_xl_s13b_b90k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 1280,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT B 16",
    },
)
def openclip_vitB16_datacomp_l(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16", pretrained="datacomp_l_s1b_b8k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 633,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT H 14",
    },
)
def openclip_vitH14(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-H-14", pretrained="laion2b_s32b_b79k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-H-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 16,
        "model_size": 86,
        "learning_objective": "XVLM",
        "architecture": "Swin",
        "name": "XVLM Swin B",
    },
)
def xvlm_flickr(model_name, **kwargs):
    from .wrappers.xvlm_util.xvlm import XVLM
    from .wrappers.xvlm_util.tokenization_bert import BertTokenizer
    from .wrappers.xvlm_util.tokenization_roberta import RobertaTokenizer
    from .wrappers.xvlm_util.utils import get_config

    config, model_path = get_config("xvlm-flickr")

    model = XVLM(config)

    model.load_pretrained(
        model_path,
        config,
        is_eval=True,
        is_pretrained=False,  # never used pretrained in NegCLIP paper?
    )

    if config["use_roberta"]:
        tokenizer = RobertaTokenizer.from_pretrained(config["text_encoder"])
    else:
        # TODO: Hack. We should use the tokenizer from the config
        tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    return XVLMModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 70,
        "model_size": 86,
        "learning_objective": "Other",
        "architecture": "vit",
        "name": "FLAVA ViT B 32",
    },
)
def flava_full(model_name, **kwargs):
    from transformers import FlavaForPreTraining, FlavaImageProcessor, BertTokenizer

    model = FlavaForPreTraining.from_pretrained("facebook/flava-full")

    processor = FlavaImageProcessor.from_pretrained("facebook/flava-full")

    tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")

    return FlavaModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=processor.size["height"],
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT L 14",
    },
)
def openclip_vitL14_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14", pretrained="laion400m_e32"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 12800,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "DataComp ViT L 14",
    },
)
def openclip_vitL14_datacomp_xl(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14", pretrained="datacomp_xl_s13b_b90k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT L 14",
    },
)
def openclip_vitL14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-L-14", pretrained="laion2b_s32b_b82k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-L-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 307,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "CLIP ViT L 14",
    },
)
def clip_vitL14(model_name, **kwargs):
    model, _ = clip.load("ViT-L/14")

    tokenizer = clip.tokenize

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 16,
        "model_size": 86,
        "learning_objective": "XVLM",
        "architecture": "Swin",
        "name": "XVLM Swin B",
    },
)
def xvlm_coco(model_name, **kwargs):
    from .wrappers.xvlm_util.xvlm import XVLM
    from .wrappers.xvlm_util.tokenization_bert import BertTokenizer
    from .wrappers.xvlm_util.tokenization_roberta import RobertaTokenizer
    from .wrappers.xvlm_util.utils import get_config

    config, model_path = get_config("xvlm-coco")

    model = XVLM(config)

    model.load_pretrained(
        model_path,
        config,
        is_eval=True,
        is_pretrained=False,  # never used pretrained in NegCLIP paper?
    )

    if config["use_roberta"]:
        tokenizer = RobertaTokenizer.from_pretrained(config["text_encoder"])
    else:
        # TODO: Hack. We should use the tokenizer from the config
        tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

    return XVLMModel(
        model=model,
        model_name=model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=384,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT B 32",
    },
)
def openclip_vitB32_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained="laion400m_e32"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT B 32",
    },
)
def openclip_vitB32_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-32", pretrained="laion2b_s34b_b79k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 1011,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT g 14",
    },
)
def openclip_vitG14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-g-14", pretrained="laion2b_s34b_b88k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-g-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 1843,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT G 14",
    },
)
def openclip_vitbigG14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-bigG-14", pretrained="laion2b_s39b_b160k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-bigG-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT B 16",
    },
)
def openclip_vitB16_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16", pretrained="laion2b_s34b_b88k"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "OpenCLIP ViT B 16",
    },
)
def openclip_vitB16_400m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "ViT-B-16", pretrained="laion400m_e32"
    )

    tokenizer = open_clip.get_tokenizer("ViT-B-16")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 307,
        "learning_objective": "Other",
        "architecture": "vit",
        "name": "OpenCOCA ViT L 14",
    },
)
def opencoca_vitL14_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "coca_ViT-L-14", pretrained="laion2b_s13b_b90k"
    )

    tokenizer = open_clip.get_tokenizer("coca_ViT-L-14")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=76,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 2000,
        "model_size": 86,
        "learning_objective": "Other",
        "architecture": "vit",
        "name": "OpenCOCA ViT B 32",
    },
)
def opencoca_vitB32_2b(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "coca_ViT-B-32", pretrained="laion2b_s13b_b90k"
    )

    tokenizer = open_clip.get_tokenizer("coca_ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        context_length=76,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Negative CLIP",
        "architecture": "vit",
        "name": "NegCLIP ViT B 32",
    },
)
def negclip_vitB32(model_name, **kwargs):
    path = os.path.join(HUB_CACHE_DIR, "negclip.pth")
    if not os.path.exists(path):
        print("Downloading the NegCLIP model...")
        import gdown

        gdown.download(id="1ooVVPxB-tvptgmHlIMMFGV3Cg-IrhbRZ", output=path, quiet=False)
    model, _, _ = open_clip.create_model_and_transforms("ViT-B-32", pretrained=path)

    tokenizer = open_clip.get_tokenizer("ViT-B-32")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size[0],
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 86,
        "learning_objective": "Contrastive",
        "architecture": "vit",
        "name": "CLIP ViT B 16",
    },
)
def clip_vitB16(model_name, **kwargs):
    model, _ = clip.load("ViT-B/16")
    tokenizer = clip.tokenize

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50",
    },
)
def clip_resnet50(model_name, **kwargs):
    model, _ = clip.load("RN50")
    tokenizer = clip.tokenize
    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50 GeLU",
    },
)
def clip_resnet50_quickgelu(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "RN50-quickgelu", pretrained="openai"
    )

    tokenizer = open_clip.get_tokenizer("RN50-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 15,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50 GeLU",
    },
)
def clip_resnet50_quickgelu_yfcc15m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "RN50-quickgelu", pretrained="yfcc15m"
    )

    tokenizer = open_clip.get_tokenizer("RN50-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 12,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50 GeLU",
    },
)
def clip_resnet50_quickgelu_cc12m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "RN50-quickgelu", pretrained="cc12m"
    )

    tokenizer = open_clip.get_tokenizer("RN50-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 15,
        "model_size": 56,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "OpenCLIP ResNet101",
    },
)
def openclip_resnet101_yfcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms("RN101", pretrained="yfcc15m")

    tokenizer = open_clip.get_tokenizer("RN101")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 15,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "OpenCLIP ResNet50",
    },
)
def openclip_resnet50_yfcc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms("RN50", pretrained="yfcc15m")

    tokenizer = open_clip.get_tokenizer("RN50")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 12,
        "model_size": 38,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "OpenCLIP ResNet50",
    },
)
def openclip_resnet50_cc(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms("RN50", pretrained="cc12m")

    tokenizer = open_clip.get_tokenizer("RN50")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 56,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet101",
    },
)
def clip_resnet101(model_name, **kwargs):
    model, _ = clip.load("RN101")
    tokenizer = clip.tokenize
    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 56,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet101 GeLU",
    },
)
def clip_resnet101_quickgelu(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "RN101-quickgelu", pretrained="openai"
    )

    tokenizer = open_clip.get_tokenizer("RN101-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 15,
        "model_size": 56,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet101 GeLU",
    },
)
def clip_resnet101_quickgelu_yfcc15m(model_name, **kwargs):
    model, _, _ = open_clip.create_model_and_transforms(
        "RN101-quickgelu", pretrained="yfcc15m"
    )

    tokenizer = open_clip.get_tokenizer("RN101-quickgelu")

    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.image_size,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 87,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50x4",
    },
)
def clip_resnet50x4(model_name, **kwargs):
    model, _ = clip.load("RN50x4")
    tokenizer = clip.tokenize
    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 167,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50x16",
    },
)
def clip_resnet50x16(model_name, **kwargs):
    model, _ = clip.load("RN50x16")
    tokenizer = clip.tokenize
    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )


@register_model(
    "vision_text",
    {
        "dataset_size": 400,
        "model_size": 420,
        "learning_objective": "Contrastive",
        "architecture": "conv",
        "name": "CLIP ResNet50x64",
    },
)
def clip_resnet50x64(model_name, **kwargs):
    model, _ = clip.load("RN50x64")
    tokenizer = clip.tokenize
    return ClipModel(
        model,
        model_name,
        tokenizer=tokenizer,
        norm_mean=timm.data.constants.OPENAI_CLIP_MEAN,
        norm_std=timm.data.constants.OPENAI_CLIP_STD,
        input_resolution=model.visual.input_resolution,
        logit_scale=model.logit_scale,
        **kwargs
    )
